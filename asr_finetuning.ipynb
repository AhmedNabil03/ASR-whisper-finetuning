{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets evaluate jiwer gradio"
      ],
      "metadata": {
        "id": "kukuGATrZUKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "52bce09bef364958a533b4d74fff8d42",
            "475da84e685e40059bee297f01c6a10d",
            "2e86e57081b04ae6baf32387bdad02ab",
            "7ffe23f1f3e4418e84bfa01225b01c93",
            "51df5d90530a43d8993eb67dc66db335",
            "27d00d42926c4e0ebe50c8c60f415e71",
            "ccf1d68bfc1a4cc28953f2949650fe76",
            "df12fcfc21d1413b989b1ad8ea329229",
            "43eb7aff09564149a0be64a11688bd43",
            "e16c41fb099e406892b033d75d63d433",
            "eda89012c5874f8f9e5f99ad12206bf4",
            "30fba289648441f19205a6cb36276a31",
            "0753a9b1dbb243269401ea2b4e4f8fc5",
            "be2703cedd694c06a5aca59ecf4933ec",
            "1137c02f7b9f4de4b005bfc59733a297",
            "afc657b5cec74c7b9da35ef954113c58",
            "c5e5fa2af85342088640a42a878deb07",
            "f2537d1a4d6b45589917cc5fde27c0d7",
            "0f015b3d50ff44f8b3e38faf1f2c178c",
            "46083aa1e63d4ef2b435954f7494bbcd"
          ]
        },
        "id": "fvU4JZHxYw2w",
        "outputId": "fe22c660-d38b-4903-ff61-3b7c956f2ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52bce09bef364958a533b4d74fff8d42"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "svFg51CRZOVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"train+validation\", ) # dv: Maldivian Language\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"dv\", split=\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9aVqCCwYwzS",
        "outputId": "afa55561-bc0f-4c60-ab18-d2dc21eed7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mQmJZBtYwxG",
        "outputId": "1fbbc3ce-15df-4874-dcbe-d0866b0f15d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 4904\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 2212\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To disregard other features, making the fine-tuning process easier\n",
        "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])\n",
        "common_voice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9FpeNztYwt0",
        "outputId": "5dbe188e-134a-4750-883b-474bd8f98903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['audio', 'sentence'],\n",
              "        num_rows: 4904\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['audio', 'sentence'],\n",
              "        num_rows: 2212\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper Model (Processor)"
      ],
      "metadata": {
        "id": "NXwlpSlZcgOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All whisper languages, which don't contain Maldivian\n",
        "# But Dhivehi (Maldivian Language) is closely related to the Sinhalese language of Sri Lanka.\n",
        "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
        "\n",
        "TO_LANGUAGE_CODE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JexoluuKYwqb",
        "outputId": "959b7e59-e169-4631-8035-a2625955addc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'english': 'en',\n",
              " 'chinese': 'zh',\n",
              " 'german': 'de',\n",
              " 'spanish': 'es',\n",
              " 'russian': 'ru',\n",
              " 'korean': 'ko',\n",
              " 'french': 'fr',\n",
              " 'japanese': 'ja',\n",
              " 'portuguese': 'pt',\n",
              " 'turkish': 'tr',\n",
              " 'polish': 'pl',\n",
              " 'catalan': 'ca',\n",
              " 'dutch': 'nl',\n",
              " 'arabic': 'ar',\n",
              " 'swedish': 'sv',\n",
              " 'italian': 'it',\n",
              " 'indonesian': 'id',\n",
              " 'hindi': 'hi',\n",
              " 'finnish': 'fi',\n",
              " 'vietnamese': 'vi',\n",
              " 'hebrew': 'he',\n",
              " 'ukrainian': 'uk',\n",
              " 'greek': 'el',\n",
              " 'malay': 'ms',\n",
              " 'czech': 'cs',\n",
              " 'romanian': 'ro',\n",
              " 'danish': 'da',\n",
              " 'hungarian': 'hu',\n",
              " 'tamil': 'ta',\n",
              " 'norwegian': 'no',\n",
              " 'thai': 'th',\n",
              " 'urdu': 'ur',\n",
              " 'croatian': 'hr',\n",
              " 'bulgarian': 'bg',\n",
              " 'lithuanian': 'lt',\n",
              " 'latin': 'la',\n",
              " 'maori': 'mi',\n",
              " 'malayalam': 'ml',\n",
              " 'welsh': 'cy',\n",
              " 'slovak': 'sk',\n",
              " 'telugu': 'te',\n",
              " 'persian': 'fa',\n",
              " 'latvian': 'lv',\n",
              " 'bengali': 'bn',\n",
              " 'serbian': 'sr',\n",
              " 'azerbaijani': 'az',\n",
              " 'slovenian': 'sl',\n",
              " 'kannada': 'kn',\n",
              " 'estonian': 'et',\n",
              " 'macedonian': 'mk',\n",
              " 'breton': 'br',\n",
              " 'basque': 'eu',\n",
              " 'icelandic': 'is',\n",
              " 'armenian': 'hy',\n",
              " 'nepali': 'ne',\n",
              " 'mongolian': 'mn',\n",
              " 'bosnian': 'bs',\n",
              " 'kazakh': 'kk',\n",
              " 'albanian': 'sq',\n",
              " 'swahili': 'sw',\n",
              " 'galician': 'gl',\n",
              " 'marathi': 'mr',\n",
              " 'punjabi': 'pa',\n",
              " 'sinhala': 'si',\n",
              " 'khmer': 'km',\n",
              " 'shona': 'sn',\n",
              " 'yoruba': 'yo',\n",
              " 'somali': 'so',\n",
              " 'afrikaans': 'af',\n",
              " 'occitan': 'oc',\n",
              " 'georgian': 'ka',\n",
              " 'belarusian': 'be',\n",
              " 'tajik': 'tg',\n",
              " 'sindhi': 'sd',\n",
              " 'gujarati': 'gu',\n",
              " 'amharic': 'am',\n",
              " 'yiddish': 'yi',\n",
              " 'lao': 'lo',\n",
              " 'uzbek': 'uz',\n",
              " 'faroese': 'fo',\n",
              " 'haitian creole': 'ht',\n",
              " 'pashto': 'ps',\n",
              " 'turkmen': 'tk',\n",
              " 'nynorsk': 'nn',\n",
              " 'maltese': 'mt',\n",
              " 'sanskrit': 'sa',\n",
              " 'luxembourgish': 'lb',\n",
              " 'myanmar': 'my',\n",
              " 'tibetan': 'bo',\n",
              " 'tagalog': 'tl',\n",
              " 'malagasy': 'mg',\n",
              " 'assamese': 'as',\n",
              " 'tatar': 'tt',\n",
              " 'hawaiian': 'haw',\n",
              " 'lingala': 'ln',\n",
              " 'hausa': 'ha',\n",
              " 'bashkir': 'ba',\n",
              " 'javanese': 'jw',\n",
              " 'sundanese': 'su',\n",
              " 'cantonese': 'yue',\n",
              " 'burmese': 'my',\n",
              " 'valencian': 'ca',\n",
              " 'flemish': 'nl',\n",
              " 'haitian': 'ht',\n",
              " 'letzeburgesch': 'lb',\n",
              " 'pushto': 'ps',\n",
              " 'panjabi': 'pa',\n",
              " 'moldavian': 'ro',\n",
              " 'moldovan': 'ro',\n",
              " 'sinhalese': 'si',\n",
              " 'castilian': 'es',\n",
              " 'mandarin': 'zh'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The WhisperProcessor contains both WhisperFeatureExtractor and WhisperTokenizer.\n",
        "# We can call it to perform both the audio pre-processing and the text token post-processing\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\", language=\"sinhalese\", task=\"transcribe\"\n",
        ")"
      ],
      "metadata": {
        "id": "pp6GuGKnYwn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Process the Data"
      ],
      "metadata": {
        "id": "Glq5WZjjdWhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"].features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCF1P5ZtYwkr",
        "outputId": "603dd3ef-9589-4eda-e286-89b5d6fae8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
              " 'sentence': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper feature extractor expects the sampling rate of 16kHz\n",
        "from datasets import Audio\n",
        "\n",
        "sampling_rate = processor.feature_extractor.sampling_rate   # 16kHz\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
      ],
      "metadata": {
        "id": "dRU0Xe6yYwiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM_NkqkbYmTC",
        "outputId": "58b7fe0e-7544-4ab3-e786-1adfcd7e3fb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
              " 'sentence': Value(dtype='string', id=None)}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "common_voice[\"train\"].features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        audio=audio[\"array\"],   # converts the raw waveform into a log-mel spectrogram\n",
        "        sampling_rate=audio[\"sampling_rate\"],    # ensures the audio sample matches the required sampling rate\n",
        "        text=example[\"sentence\"],   # converts the text transcription into tokenized label IDs.\n",
        "    )\n",
        "\n",
        "    # compute input length of audio sample in seconds\n",
        "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "BzdT9boKeL7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(\n",
        "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
        ")"
      ],
      "metadata": {
        "id": "8N3norkSeL34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNX4g_JyeL1P",
        "outputId": "44f9bb25-81e8-4e47-fe63-cff55d72fffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_features', 'labels', 'input_length'],\n",
              "        num_rows: 4904\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_features', 'labels', 'input_length'],\n",
              "        num_rows: 2212\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To ensure the stability of training, we filter any training data with audio samples longer than 30s\n",
        "max_input_length = 30.0\n",
        "\n",
        "def is_audio_in_length_range(length):\n",
        "    return length < max_input_length"
      ],
      "metadata": {
        "id": "WoKzVX5ueLyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
        "    is_audio_in_length_range,\n",
        "    input_columns=[\"input_length\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Ko-rvFTpeLwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyLahJ3Og_4Y",
        "outputId": "ae43eae2-10a8-4f61-9c37-48f88c50b1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_features', 'labels', 'input_length'],\n",
              "    num_rows: 4904\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator Definition"
      ],
      "metadata": {
        "id": "D8UUY60IkBI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [\n",
        "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
        "        ]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")    # converts spectrograms into a PyTorch tensor.\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "q9KG2smPjvgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "BnwJRcvVjvc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "EgDxQ8mtktiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Error Rate (WER)\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "jrsYsmYZjvap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text normalizer removes punctuation, casing (uppercase/lowercase differences), any formatting ensures that WER is not affected by minor formatting differences.\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()"
      ],
      "metadata": {
        "id": "pSSqvXiom6_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # compute orthographic wer\n",
        "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    # compute normalised WER\n",
        "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
        "    label_str_norm = [normalizer(label) for label in label_str]\n",
        "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
        "    pred_str_norm = [\n",
        "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str_norm[i]\n",
        "        for i in range(len(label_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
      ],
      "metadata": {
        "id": "WfTcS0XRjvYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a Pre-Trained Checkpoint"
      ],
      "metadata": {
        "id": "wBh9VwoGmmY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "C6W3M3-Qg_w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# set language and task for generation and re-enable cache\n",
        "model.generate = partial(\n",
        "    model.generate, language=\"sinhalese\", task=\"transcribe\", use_cache=True\n",
        ")"
      ],
      "metadata": {
        "id": "6KeoOqjog_u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Configuration"
      ],
      "metadata": {
        "id": "8PJY5DVjoIoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-dv\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_steps=50,\n",
        "    max_steps=500,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpDn3WVmoLXt",
        "outputId": "1c1e0eb2-e187-4c44-b505-41799e896b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsBKebdeoLUT",
        "outputId": "7bb7984d-226b-408d-8ee5-f994fbe6a129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-6add8000ae4f>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0xIhNYxqoLRw",
        "outputId": "e0e7e69d-4476-45ef-de85-190d06141f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 43:36, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer Ortho</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.165700</td>\n",
              "      <td>0.191221</td>\n",
              "      <td>65.721847</td>\n",
              "      <td>14.166985</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2758: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.9434465951919556, metrics={'train_runtime': 2621.3716, 'train_samples_per_second': 1.526, 'train_steps_per_second': 0.191, 'total_flos': 1.15434160128e+18, 'train_loss': 0.9434465951919556, 'epoch': 0.8156606851549756})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sharing Results"
      ],
      "metadata": {
        "id": "QPUtjB2Tp9a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
        "    \"dataset\": \"Common Voice 13\",\n",
        "    \"language\": \"dv\",\n",
        "    \"model_name\": \"Whisper Small Dv - Sanchit Gandhi\",\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ],
      "metadata": {
        "id": "unS8GeP6p6Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "sSHj1o-0p6DJ",
        "outputId": "9efe36ac-c19f-40a0-f576-820c8ad17a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/AhmedNabil1/whisper-small-dv/commit/ff26163a19f816f9178a78578aa643a928dc5b85', commit_message='End of training', commit_description='', oid='ff26163a19f816f9178a78578aa643a928dc5b85', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AhmedNabil1/whisper-small-dv', endpoint='https://huggingface.co', repo_type='model', repo_id='AhmedNabil1/whisper-small-dv'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a demo with Gradio"
      ],
      "metadata": {
        "id": "qtlOIerfsKh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"AhmedNabil1/whisper-small-dv\"\n",
        "pipe = pipeline(\"automatic-speech-recognition\", model=model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354,
          "referenced_widgets": [
            "8826bf2029b34167846288aad36a9cee",
            "b1ca49c387f047a8b720938a2a441dcf",
            "43e6c40b8a9f4e1b87aa132001837708",
            "f3838531d056474a961b9662d5c0e3e0",
            "ab31852cf97b4b1b80c80ac8d14f9d50",
            "2cd2241367b546e9bfafae09d8579f62",
            "848c9a8bfd2d496493be7c8b5fdb06a9",
            "e2a44ea3e6894a8fb0a5a5a34d52a9f3",
            "7cb1e9ca14414c1c8403d51cde956d0a",
            "5b5c3824de034a59ab4522ff4d786782",
            "a8e2d3e7c4e14b6498ffb330492de1ae",
            "43d44ffc248948bc895c2bd1088730be",
            "0a4f49c5aadb4352bd084cedc9e848f7",
            "64cdd271f7bd4fd39a6e8c6b5b754f2a",
            "09fd36447281483a9c8c6e4f75f1beca",
            "01c476c727fb4700a733a8f37d91c66e",
            "d4db9952479a498bbcec61b3a5829566",
            "c53ac2dcdba7416c8378e37130921368",
            "fcf5eeb1b7184615a3803f4c151289d9",
            "dba3d43d1ce647d3a431989cf82935e5",
            "002567f0dbe8463d9fbdbe5a67ff1db9",
            "222e9db35b4d4ccaac13cd2a31bd6987",
            "91bc56803deb478e8ec8f61b370a0253",
            "f5fe479585f5451e84a2f7b36c7c8044",
            "ae9413c50f9b4ccc89bd0f487b3a36d6",
            "a482e347efbe4573a85bf728c0d3d463",
            "7e1f5fa53a0e48bb9649477ef9b81c26",
            "8e1379f1f357460c9c7c27e60a0ec9e6",
            "ecd18507eb7340fc86a42d7f3923b489",
            "edddcfe89cb54965a6f10df8d50fd399",
            "03672949d577456b954c8d7ed8efe5ea",
            "8b202ac0eabf4afda296fbb4ef34b29f",
            "05e3d35bc81947748968b077fd19fd88",
            "aeb8e6bf8ae848dbb962bd6caf65a557",
            "c99318c3a94046c2a3bd7d4ac1f133a6",
            "bd001a7846aa4fa9848de2a7d463d256",
            "97a48272b92d4654be355fed08ae5730",
            "628a1782dc6543a58daf7d136a924c80",
            "034b76f629c14e208fb67a1eae7bb291",
            "6ee34d826e4a4d2d912bc2076913925a",
            "7cc1424b71004c7b8f4072c897aa8c85",
            "b474efb57b01424d9bbd63039d166f8a",
            "0c0ac519161b4dd087cbac0014f64411",
            "46c28b0400ea43a291a4c226c7d415be",
            "005091a30d7340c5906509cc6a112013",
            "2605a31a643044c998c5dce63ae56d5d",
            "c5d20fc35b994172b16f4bd31d0e3d2a",
            "36145a3b4cbc4c2e8b55a8e61d6d8aa1",
            "6b4f6aeda2934c619141c27f6843b91f",
            "386a0d7c9d2c450b89349c09f9268552",
            "ea600e71e1494a7ab40206833804b64b",
            "43403d36d9894ed4b88d119f7bc79055",
            "b377e898bed14237abc7b5b4fe8910bd",
            "3f53dd4fa4434179bf426a84cb905137",
            "1c7c9e4cab9740da9443cf37026e0303",
            "fa7511aaf22f47a9ac47c77d3d9ef5a6",
            "fe263b7a841a4d6bb051e89358dbd91a",
            "6b457a07b81844b9bf37ea90ac93f2eb",
            "abfb3a2351794ff589a8ce4d066eec31",
            "323a261c497d441bb9d67535e60c2072",
            "cbd5a535f22342d88e0f1cd76759f6a2",
            "424781f0a2474312a0fd7d9a92340791",
            "43a304652c7e4b1999020520fbd63d95",
            "17ad220f695644719ec89534a445519c",
            "81a175de4e1e4f908a49ecfabf612a30",
            "64ecd54d74bb48728a21a4daa1745460",
            "6249a0fff9784152a05d616cf08db498",
            "a6ad16b232e44d999754e74670252017",
            "ff6b5ee46ab04669ade2d5ffc3ecf3a2",
            "5ab7a72c971a4651bec77a3e2afa0842",
            "aab36f86697c4bc59cbf4188f4d093ef",
            "80e2b46741b74182b5bf441d5a79df4b",
            "8a25559dc5ff48c6ad94c843f31f5884",
            "b732b38d5d984bccad01a5dbe903c8d3",
            "cbfed271ae414b53be1fa8310e0b5581",
            "ffcaf4c905734b15b62da62de13e9ef9",
            "9485329bdfc943a6a27446e48e3db52f",
            "3072b5c4902a44268e64c7b740c3356c",
            "cda1acb594b048bba163b9f37ce354f8",
            "7a355a4125f643778ba1083c2df26ee9",
            "ff1fa85aedbb41b7be30b8b2d6891b3a",
            "96b0afa1287840c59fdd10b8b47d09cb",
            "af9194c5cfbe4f59838ae434a5a583c9",
            "e185df55f1f441a2b6e7d84ff4fc2888",
            "1a3f6daed23840f5865a6487c09a0253",
            "1cb217fa58844da6835bef24e5e7d822",
            "51c618a76b2743b5931aba046bc83564",
            "6d019defe28844598a040c9e3d6d7a2e",
            "ea185e5e56304a7b832e4955178c3aef",
            "12cc36093e1044ca8b1b14b312d77159",
            "dd60fb6d4a794bffbb5a63b88a56dc68",
            "34c2c6cf7b164f8d9aa07c5aa1187d15",
            "615cbc6c67c845b18a5e237979dccc4d",
            "97be2aece8d0499f82f4921514cb9cbf",
            "f20c7e9b7b9c4c3f8725640c32efe73f",
            "89621328803148589a40c22dbcb09f46",
            "15ded943f7d2497bae91cdc5a8bff70a",
            "bb48d333b2c84281b5d68ae92e1f8c84",
            "322f1d875a544ce89f0ae12b40cb689c",
            "0dd6ceea7a7c435bb6cbc6de244d4abc",
            "c4644b5e35ca4dd3be399054a86e4dc7",
            "21b20cea523a459ab3d4072ab1e08e5d",
            "029b6a5453d543098ebbf3f27d511854",
            "afe2a8546c0b460c9904f9dcdad23d31",
            "13db4519019d490492ffb7819769c4c5",
            "5ebfeebea8f54aad876658cb81545f61",
            "53cc2e4b07de4660b4bb11ed7f651549",
            "f4a03ce32b3f4d21b547e0681e69fddb",
            "ab77b1d78e814dac9d168620d5df8301",
            "3ab4c2a702d4462b938d552c38df69cd"
          ]
        },
        "id": "9uoXI3L7sLuc",
        "outputId": "8c33b832-2e74-46fe-f4e8-4363ec1bbd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8826bf2029b34167846288aad36a9cee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43d44ffc248948bc895c2bd1088730be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91bc56803deb478e8ec8f61b370a0253"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeb8e6bf8ae848dbb962bd6caf65a557"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "005091a30d7340c5906509cc6a112013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa7511aaf22f47a9ac47c77d3d9ef5a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6249a0fff9784152a05d616cf08db498"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3072b5c4902a44268e64c7b740c3356c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea185e5e56304a7b832e4955178c3aef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dd6ceea7a7c435bb6cbc6de244d4abc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_speech(filepath):\n",
        "    output = pipe(\n",
        "        filepath,\n",
        "        max_new_tokens=256,\n",
        "        generate_kwargs={\n",
        "            \"task\": \"transcribe\",\n",
        "            \"language\": \"sinhalese\",\n",
        "        },\n",
        "        chunk_length_s=30,\n",
        "        batch_size=8,\n",
        "    )\n",
        "    return output[\"text\"]"
      ],
      "metadata": {
        "id": "qfa_vr4DsLrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "mic_transcribe = gr.Interface(\n",
        "    fn=transcribe_speech,\n",
        "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
        "    outputs=gr.components.Textbox(),\n",
        ")\n",
        "\n",
        "file_transcribe = gr.Interface(\n",
        "    fn=transcribe_speech,\n",
        "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
        "    outputs=gr.components.Textbox(),\n",
        ")"
      ],
      "metadata": {
        "id": "9UBK5AtJsLom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with demo:\n",
        "    gr.TabbedInterface(\n",
        "        [mic_transcribe, file_transcribe],\n",
        "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "-nYF5jEzsLmJ",
        "outputId": "fc0f868b-cf3c-46e1-ab43-eb0bf5fcfc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3469becd5a8122eb5a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3469becd5a8122eb5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3469becd5a8122eb5a.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fc2pDBAGg_ql"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
